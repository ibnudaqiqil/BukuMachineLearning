{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Studi Kasus Naive Bayes 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analisa sentimen bahasa indonesia menggunakan Multinominal Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1. Pembersihan data (Pre-processing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import regex\n",
    "import re\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory, StopWordRemover, ArrayDictionary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Buat Fungsi penghapusan\n",
    "\n",
    "    URLs\n",
    "    Hashtags\n",
    "    Mentions\n",
    "    Emojis dan  Smileys\n",
    "    Stemming\n",
    "    Remove Stopword dan Reserved words (RT, FAV)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#start process_tweet\n",
    "def preprocessTweet(tweet):\n",
    "    #konversi menjadi huruf kecil\n",
    "    tweet = tweet.lower()\n",
    "    #konversi www.* atau https?://* menjadi URL\n",
    "    tweet = re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+))','URL',tweet)\n",
    "    \n",
    "    #Ganti #tagar menjadi tagar\n",
    "    tweet = re.sub(r'#([^\\s]+)', r'\\1', tweet)\n",
    "    \n",
    "    #konversi mention @username mejadi AT_USER\n",
    "    tweet = re.sub('@[^\\s]+','AT_USER',tweet)\n",
    "     # keep only words\n",
    "    tweet = re.sub(\"[^a-zA-Z]\", \" \", tweet) \n",
    "    #Buang  white spaces\n",
    "    tweet = re.sub('[\\s]+', ' ', tweet)\n",
    "\n",
    "    #trim\n",
    "    tweet = tweet.strip('\\'\"')\n",
    "  \n",
    "    #setelah bersih di stem\n",
    "    stemmerFactory = StemmerFactory()\n",
    "    stemmer = stemmerFactory.create_stemmer()\n",
    "    tweet   = stemmer.stem(tweet)\n",
    "    \n",
    "    \n",
    "    # Ambil Stopword bawaan\n",
    "    stopwordDefault = StopWordRemoverFactory().get_stop_words()\n",
    "    stopwordTambahan = ['RT', 'lol']\n",
    "\n",
    "    # Merge stopword\n",
    "    dataStopword = stopwordDefault + stopwordTambahan\n",
    "\n",
    "    dictionary = ArrayDictionary(dataStopword)\n",
    "    stopRemover = StopWordRemover(dictionary)\n",
    "\n",
    "    tweet = (stopRemover.remove(tweet))\n",
    "\n",
    "\n",
    "    return tweet\n",
    "#end\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ujicoba Preprosessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuhan sangat cinta negeri indah ini. Tuhan selalu mengirimkan pemimpin yg hebat. Dukungan penuh dari rakyatlah yg akan memampukan pemerintahan Jokowi untuk terus membangun. Mari kita dukung beliau dgn sepenuh hati. #JokowiMaruf2019 #jokowi2019sekalilagi #TetapJokowi \n",
      "\n",
      "\n",
      "tuhan cinta negeri indah tuhan kirim pimpin yg hebat dukung penuh rakyat yg perintah jokowi bangun mari dukung beliau dgn sepenuh hati jokowimaruf jokowi sekalilagi tetapjokowi\n"
     ]
    }
   ],
   "source": [
    "tw1=\"Tuhan sangat cinta negeri indah ini. Tuhan selalu mengirimkan pemimpin yg hebat. Dukungan penuh dari rakyatlah yg akan memampukan pemerintahan Jokowi untuk terus membangun. Mari kita dukung beliau dgn sepenuh hati. #JokowiMaruf2019 #jokowi2019sekalilagi #TetapJokowi\"\n",
    "print (tw1,\"\\n\\n\")\n",
    "print (preprocessTweet(tw1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "positive tuhan cinta negeri indah tuhan kirim pimpin yg hebat dukung penuh rakyat yg perintah jokowi bangun mari dukung beliau dgn sepenuh hati jokowimaruf jokowi sekalilagi tetapjokowi\n",
      "negatif binatang pegang tali manusia pegang janji bunyi pepatah janji sekira sanggup penuh janji capres ri realisasi janji hutang mudah umbar janji\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#Baca data tweet\n",
    "fp = open('data/dataset.csv', 'r')\n",
    "line = fp.readline()\n",
    "\n",
    "while line:\n",
    "    processedTweet = preprocessTweet(line)\n",
    "    print (processedTweet)\n",
    "    line = fp.readline()\n",
    "#end loop\n",
    "fp.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BagOfWords(object):\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.__bag_of_words = {}\n",
    "        \n",
    "    def add(self,word):\n",
    "        if word in self.__bag_of_words:\n",
    "            self.__bag_of_words[word] += 1\n",
    "        else:\n",
    "            self.__bag_of_words[word] = 1\n",
    "    \n",
    "    def len(self):\n",
    "        \"\"\" Returning the number of different words of an object \"\"\"\n",
    "        return len(self.__bag_of_words)\n",
    "    \n",
    "    def Words(self):\n",
    "        \"\"\" Returning a list of the words contained in the object \"\"\"\n",
    "        return self.__bag_of_words.keys()\n",
    "    \n",
    "        \n",
    "    def BagOfWords(self):\n",
    "        \"\"\" Returning the dictionary, containing the words (keys) with their frequency (values)\"\"\"\n",
    "        return self.__bag_of_words\n",
    "        \n",
    "    def WordFreq(self,word):\n",
    "        \"\"\" Returning the frequency of a word \"\"\"\n",
    "        if word in self.__bag_of_words:\n",
    "            return self.__bag_of_words[word]\n",
    "        else:\n",
    "            return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'kucing': 2, 'kuda': 1, 'anjing': 1}\n",
      "dict_keys(['kucing', 'kuda', 'anjing'])\n"
     ]
    }
   ],
   "source": [
    "kata2 = BagOfWords()\n",
    "kata2.add(\"kucing\")\n",
    "kata2.add(\"kuda\")\n",
    "kata2.add(\"anjing\")\n",
    "kata2.add(\"kucing\")\n",
    "\n",
    "print (kata2.BagOfWords())\n",
    "print (kata2.Words())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Document(object):\n",
    "    \"\"\" Used both for learning (training) documents and for testing documents. The optional parameter lear\n",
    "    has to be set to True, if a classificator should be trained. If it is a test document learn has to be set to False. \"\"\"\n",
    "    _vocabulary = BagOfWords()\n",
    " \n",
    "    def __init__(self, vocabulary):\n",
    "        self.__name = \"\"\n",
    "        self.__document_class = None\n",
    "        self._words_and_freq = BagOfWords()\n",
    "        Document._vocabulary = vocabulary\n",
    "    \n",
    "    def read_document(self,filename, learn=False):\n",
    "        \"\"\" A document is read. It is assumed that the document is either encoded in utf-8 or in iso-8859... (latin-1).\n",
    "        The words of the document are stored in a Bag of Words, i.e. self._words_and_freq = BagOfWords() \"\"\"\n",
    "        try:\n",
    "            text = open(filename,\"r\", encoding='utf-8').read()\n",
    "        except UnicodeDecodeError:\n",
    "            text = open(filename,\"r\", encoding='latin-1').read()\n",
    "        text = text.lower()\n",
    "        self.set_document(text,learn)\n",
    "        \n",
    "    def set_document(self,text, learn=False):\n",
    "        words = re.split(r\"\\W\",text)\n",
    "\n",
    "        self._number_of_words = 0\n",
    "        for word in words:\n",
    "            self._words_and_freq.add(word)\n",
    "            if learn:\n",
    "                Document._vocabulary.add(word)\n",
    "\n",
    "\n",
    "    def vocabulary_length(self):\n",
    "        \"\"\" Returning the length of the vocabulary \"\"\"\n",
    "        return len(Document._vocabulary)\n",
    "                \n",
    "    def WordsAndFreq(self):\n",
    "        \"\"\" Returning the dictionary, containing the words (keys) with their frequency (values) as contained\n",
    "        in the BagOfWords attribute of the document\"\"\"\n",
    "        return self._words_and_freq.BagOfWords()\n",
    "        \n",
    "    def Words(self):\n",
    "        \"\"\" Returning the words of the Document object \"\"\"\n",
    "        d =  self._words_and_freq.BagOfWords()\n",
    "        return d.keys()\n",
    "    \n",
    "    def WordFreq(self,word):\n",
    "        \"\"\" Returning the number of times the word \"word\" appeared in the document \"\"\"\n",
    "        bow =  self._words_and_freq.BagOfWords()\n",
    "        if word in bow:\n",
    "            return bow[word]\n",
    "        else:\n",
    "            return 0\n",
    "                \n",
    "    def __and__(self, other):\n",
    "        \"\"\" Intersection of two documents. A list of words occuring in both documents is returned \"\"\"\n",
    "        intersection = []\n",
    "        words1 = self.Words()\n",
    "        for word in other.Words():\n",
    "            if word in words1:\n",
    "                intersection += [word]\n",
    "        return intersection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'negatif': 1, 'binatang': 1, 'pegang': 2, 'tali': 1, 'manusia': 1, 'janji': 5, 'bunyi': 1, 'pepatah': 1, 'sekira': 1, 'sanggup': 1, 'penuh': 1, 'capres': 1, 'ri': 1, 'realisasi': 1, 'hutang': 1, 'mudah': 1, 'umbar': 1}\n"
     ]
    }
   ],
   "source": [
    "doku = Document(\"positif\")\n",
    "doku.set_document(processedTweet)\n",
    "print(doku.WordsAndFreq())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DocumentClass(Document):\n",
    "    def __init__(self, vocabulary):\n",
    "        Document.__init__(self, vocabulary)\n",
    "        self._number_of_docs = 0\n",
    "\n",
    "    def Probability(self,word):\n",
    "        \"\"\" returns the probabilty of the word \"word\" given the class \"self\" \"\"\"\n",
    "        voc_len = Document._vocabulary.len()\n",
    "        SumN = 0\n",
    "        for i in range(voc_len):\n",
    "            SumN = DocumentClass._vocabulary.WordFreq(word)\n",
    "        N = self._words_and_freq.WordFreq(word)\n",
    "        erg = 1 + N\n",
    "        erg /= voc_len + SumN\n",
    "        return erg\n",
    "\n",
    "    def __add__(self,other):\n",
    "        \"\"\" Overloading the \"+\" operator. Adding two DocumentClass objects consists in adding the \n",
    "        BagOfWords of the DocumentClass objectss \"\"\"\n",
    "        res = DocumentClass(self._vocabulary)\n",
    "        res._words_and_freq = self._words_and_freq + other._words_and_freq \n",
    " \n",
    "        return res\n",
    "\n",
    "    def SetNumberOfDocs(self, number):\n",
    "        self._number_of_docs = number\n",
    "    \n",
    "    def NumberOfDocuments(self):\n",
    "        return self._number_of_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doku_kelas = DocumentClass()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Pool(object):\n",
    "    def __init__(self):\n",
    "        self.__document_classes = {}\n",
    "        self.__vocabulary = BagOfWords()\n",
    "            \n",
    "    def sum_words_in_class(self, dclass):\n",
    "        \"\"\" The number of times all different words of a dclass appear in a class \"\"\"\n",
    "        sum = 0\n",
    "        for word in self.__vocabulary.Words():\n",
    "            WaF = self.__document_classes[dclass].WordsAndFreq()\n",
    "            if word in WaF:\n",
    "                sum +=  WaF[word]\n",
    "        return sum\n",
    "    \n",
    "    def learn(self, directory, dclass_name):\n",
    "        \"\"\" directory is a path, where the files of the class with the name dclass_name can be found \"\"\"\n",
    "        x = DocumentClass(self.__vocabulary)\n",
    "        dir = os.listdir(directory)\n",
    "        for file in dir:\n",
    "            d = Document(self.__vocabulary)\n",
    "            print(directory + \"/\" + file)\n",
    "            d.read_document(directory + \"/\" +  file, learn = True)\n",
    "            x = x + d\n",
    "        self.__document_classes[dclass_name] = x\n",
    "        x.SetNumberOfDocs(len(dir))\n",
    "\n",
    "    \n",
    "    def Probability(self, doc, dclass = \"\"):\n",
    "        \"\"\"Calculates the probability for a class dclass given a document doc\"\"\"\n",
    "        if dclass:\n",
    "            sum_dclass = self.sum_words_in_class(dclass)\n",
    "            prob = 0\n",
    "        \n",
    "            d = Document(self.__vocabulary)\n",
    "            d.read_document(doc)\n",
    "\n",
    "            for j in self.__document_classes:\n",
    "                sum_j = self.sum_words_in_class(j)\n",
    "                prod = 1\n",
    "                for i in d.Words():\n",
    "                    wf_dclass = 1 + self.__document_classes[dclass].WordFreq(i)\n",
    "                    wf = 1 + self.__document_classes[j].WordFreq(i)\n",
    "                    r = wf * sum_dclass / (wf_dclass * sum_j)\n",
    "                    prod *= r\n",
    "                prob += prod * self.__document_classes[j].NumberOfDocuments() / self.__document_classes[dclass].NumberOfDocuments()\n",
    "            if prob != 0:\n",
    "                return 1 / prob\n",
    "            else:\n",
    "                return -1\n",
    "        else:\n",
    "            prob_list = []\n",
    "            for dclass in self.__document_classes:\n",
    "                prob = self.Probability(doc, dclass)\n",
    "                prob_list.append([dclass,prob])\n",
    "            prob_list.sort(key = lambda x: x[1], reverse = True)\n",
    "            return prob_list\n",
    "\n",
    "    def DocumentIntersectionWithClasses(self, doc_name):\n",
    "        res = [doc_name]\n",
    "        for dc in self.__document_classes:\n",
    "            d = Document(self.__vocabulary)\n",
    "            d.read_document(doc_name, learn=False)\n",
    "            o = self.__document_classes[dc] &  d\n",
    "            intersection_ratio = len(o) / len(d.Words())\n",
    "            res += (dc, intersection_ratio)\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'os' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-32-ea24629c3e95>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPool\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mDClasses\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m     \u001b[0mp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbase\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-30-5e44ecdec0cb>\u001b[0m in \u001b[0;36mlearn\u001b[1;34m(self, directory, dclass_name)\u001b[0m\n\u001b[0;32m     16\u001b[0m         \u001b[1;34m\"\"\" directory is a path, where the files of the class with the name dclass_name can be found \"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDocumentClass\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__vocabulary\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m         \u001b[0mdir\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mfile\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdir\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m             \u001b[0md\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDocument\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__vocabulary\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'os' is not defined"
     ]
    }
   ],
   "source": [
    "DClasses = [\"clinton\",  \"lawyer\",  \"math\",  \"medical\",  \"music\",  \"sex\"]\n",
    "\n",
    "base = \"data/\"\n",
    "p = Pool()\n",
    "for i in DClasses:\n",
    "    p.learn(base + i, i)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
